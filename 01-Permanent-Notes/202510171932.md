# 2025-10-17 19:32

## Contexto

Machine Learning = Give the computer the ability to learn without begin explicilty programer

Supervised Leaarning - 1 and 2 ccourse
Unsuperised Learning - course 3
Reinforcemente learning 

Pratical advice for applying learning


-- Supervised Learning

Give examples with correct asnwers

Example
Input x - spam ? - Spam filtering
audio - text transcripts - speech recongtion

Regression: House price prediciton based com the house size
Predict a number infinitely many possibile

Classification - Breast cancer detection - Prediciations smals numbers 1 or 0, not the inifitely many posibles
Predict categories, predict a image its a dog ou cat
Small numbers of predications
Two or more inputs

-- Unsupervised learning
find something interesintg in unlabeled data
what type of data os structures
Clustering data - ex find some articles related
Group data

Anomaly detection - find unusual data points.
Dimensionality reduction - compress data using fewers numbers.

--- Jupyter Notebooks


--- LInear Regression With One variable
The most used model today

Regression model predicts numbers
Linear will be a line in the graph and find the exact price of house proce x size
Terminology - 
Data used to train the model, training set
NOtation x = input variable
output variable
target variable

m = number of training examples
(x,y) = single trainig examples
ith trainig example
(x(i), y(i))

Training set - features, targets
Learning algorithm
Function = Hypothesis y-hat
feature model prediction (estimated y)

How to represent f? (This example a simple LINE)

fw,b(x) = wx + b
f(x)

|
| x
|_________

Linear regression with one variable size = Unvariate


------

Cost function
How well the model is going

Coeficientes
Weights

W,B: parameters

w = 0, b = 1.5

f(x) = 0.x + 1.5
ŷ = 1.5


Cost funciont 
(ŷ - y )

fw(x) | 
for fixed w, function x input

j(w)
function of w parameter


Erro quadrático médio (MSE): Usada em tarefas de regressão, calcula a diferença quadrática média entre os valores previstos e reais. É ideal para problemas onde o erro em qualquer ponto é igualmente importante.


-------------
Gradient Descent

minJ(w,b)

Outline start with some w,b (ex w=0,b=0)
kepp change w,b to reduce(j(w,b))

Gradiente descent algorithm

w = w - A a/aJ(w,b)

A = learning rate (how big the step i go to the downhill (example of hill))

b = b - A a/abJ(w,b)

Need to be simultaneously update in w and b

Learning Rate

if a is to small, you will take a baby step to small, gradiente descent may be slow.

if a is to large, the cost can be worst, gradiente descent may overshoot, never reach the minimum

Near a local minimum, derivatice becomes smaller, Update Steps Become smaller can reach minimun wihtout decreasing learning rate

-- Gradiente Descent for linear Regression


lead to the global minimum
squared error cost have a global minium - Convex function = Bowl shape


Batch gradiente descente = each step of gradiente descent uses all the training examples




## Referências

## Tags

[[Algoritmos]]

---
*Criado em: 2025-10-17 19:32*